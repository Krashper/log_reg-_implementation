{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265f7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7926448",
   "metadata": {},
   "source": [
    "# Загрузка данных и их обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f780e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 50 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  100 non-null    float64\n",
      " 1   sepal width (cm)   100 non-null    float64\n",
      " 2   petal length (cm)  100 non-null    float64\n",
      " 3   petal width (cm)   100 non-null    float64\n",
      " 4   target             100 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 4.3 KB\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "\n",
    "data = data.loc[(data[\"target\"] == 1) | (data[\"target\"] == 2)] # выбор нужных классов ириса\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1738c61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "50                7.0               3.2                4.7               1.4   \n",
       "51                6.4               3.2                4.5               1.5   \n",
       "52                6.9               3.1                4.9               1.5   \n",
       "53                5.5               2.3                4.0               1.3   \n",
       "54                6.5               2.8                4.6               1.5   \n",
       "\n",
       "    target  \n",
       "50       1  \n",
       "51       1  \n",
       "52       1  \n",
       "53       1  \n",
       "54       1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acaee77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop([\"target\"], axis=1)\n",
    "Y = data[\"target\"].map({2: 1, 1: 0})\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3758e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f413f",
   "metadata": {},
   "source": [
    "# Реализация класса логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94127bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cross_entropy(self, h0, y, m):\n",
    "        h0 = np.clip(h0, 0.0001, 0.9999)\n",
    "        return np.sum(-y * np.log(h0) - (1 - y) * np.log(1 - h0))/m\n",
    "\n",
    "    def fit_gradient(self, X_train, Y_train, num_iterations=10000):\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values\n",
    "        X_train = X_train.T\n",
    "        Y_train = Y_train.reshape(1, X_train.shape[1])\n",
    "        m = X_train.shape[1]\n",
    "        n = X_train.shape[0]\n",
    "        self.weight = np.zeros((n,1))\n",
    "        self.bias = 0\n",
    "        cost_list = []\n",
    "        for i in range(num_iterations):\n",
    "            z = np.dot(self.weight.T, X_train) + self.bias\n",
    "            h0 = self.sigmoid(z)\n",
    "            update_weight = np.dot(h0 - Y_train, X_train.T) / m\n",
    "            update_bias = np.sum(h0 - Y_train) / m\n",
    "            self.weight = self.weight - self.learning_rate * update_weight.T\n",
    "            self.bias = self.bias - self.learning_rate * update_bias\n",
    "            if i % 1000 == 0:\n",
    "                cost = self.cross_entropy(h0, Y_train, m)\n",
    "                cost_list.append(cost)\n",
    "                print(f\"После {i} итерации значение функции стоимости = {cost}\")\n",
    "                print(self.weight, self.bias)\n",
    "        return cost_list\n",
    "\n",
    "    def fit_rmsprop(self, X_train, Y_train, beta=0.9, eps=1e-8, num_iterations=1000):\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values\n",
    "        X_train = X_train.T\n",
    "        Y_train = Y_train.reshape(1, X_train.shape[1])\n",
    "        m = X_train.shape[1]\n",
    "        n = X_train.shape[0]\n",
    "        self.weight = np.random.randn(1, n)\n",
    "        self.bias = 0\n",
    "        self.v_dw = np.zeros_like(self.weight)\n",
    "        self.v_db = 0\n",
    "        cost_list = []\n",
    "        for i in range(num_iterations):\n",
    "            z = np.dot(self.weight, X_train) + self.bias\n",
    "            h0 = self.sigmoid(z)\n",
    "            dw = np.dot(h0 - Y_train, X_train.T) / m\n",
    "            db = np.sum(h0 - Y_train) / m\n",
    "            self.v_dw = beta * self.v_dw + (1 - beta) * dw**2\n",
    "            self.v_db = beta * self.v_db + (1 - beta) * db**2\n",
    "            self.weight -= self.learning_rate * dw / (np.sqrt(self.v_dw) + eps)\n",
    "            self.bias -= self.learning_rate * db / (np.sqrt(self.v_db) + eps)\n",
    "            if i % 1000 == 0:\n",
    "                cost = self.cross_entropy(h0, Y_train, m)\n",
    "                cost_list.append(cost)\n",
    "                print(f\"После {i} итерации значение функции стоимости = {cost}\")\n",
    "                print(\"Weight: \", self.weight)\n",
    "                print(\"Bias: \", self.bias)\n",
    "                print(h0)\n",
    "        self.weight = self.weight.T\n",
    "        return cost_list\n",
    "    \n",
    "    \n",
    "    def predict(self, X_pred):\n",
    "        X_pred = X_pred.values\n",
    "        X_pred = X_pred.T\n",
    "        z = np.dot(self.weight.T, X_pred) + self.bias\n",
    "        h0 = self.sigmoid(z)\n",
    "        print(\"H0: \", h0)\n",
    "        Y_pred = np.zeros((1, X_pred.shape[1]))\n",
    "        for i in range(h0.shape[1]):\n",
    "            if h0[0, i] <= 0.5:\n",
    "                Y_pred[0, i] = 0\n",
    "            else:\n",
    "                Y_pred[0, i] = 1\n",
    "        Y_pred = Y_pred.flatten()\n",
    "        Y_pred = pd.DataFrame({\"Result\": Y_pred})\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44e49d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После 0 итерации значение функции стоимости = 0.6931471805599454\n",
      "[[0.0034    ]\n",
      " [0.00127143]\n",
      " [0.00475   ]\n",
      " [0.00231429]] 0.00028571428571428574\n",
      "После 1000 итерации значение функции стоимости = 0.40869143290929394\n",
      "[[-0.67915507]\n",
      " [-0.58448824]\n",
      " [ 1.01759466]\n",
      " [ 0.86969162]] -0.31433885063613604\n",
      "После 2000 итерации значение функции стоимости = 0.2991430927752731\n",
      "[[-1.09843603]\n",
      " [-0.98651859]\n",
      " [ 1.63920377]\n",
      " [ 1.42600597]] -0.5344479715681404\n",
      "После 3000 итерации значение функции стоимости = 0.24227154245542323\n",
      "[[-1.38432854]\n",
      " [-1.29469355]\n",
      " [ 2.07970348]\n",
      " [ 1.83148578]] -0.7067284798733726\n",
      "После 4000 итерации значение функции стоимости = 0.20733484763717766\n",
      "[[-1.59679019]\n",
      " [-1.54858962]\n",
      " [ 2.41925464]\n",
      " [ 2.15098003]] -0.8514609642739828\n",
      "После 5000 итерации значение функции стоимости = 0.18349465725560563\n",
      "[[-1.76366813]\n",
      " [-1.76696394]\n",
      " [ 2.69535658]\n",
      " [ 2.4153782 ]] -0.9782450276660934\n",
      "После 6000 итерации значение функции стоимости = 0.16604476748336885\n",
      "[[-1.89979822]\n",
      " [-1.96010663]\n",
      " [ 2.92812807]\n",
      " [ 2.64149735]] -1.092317943881251\n",
      "После 7000 итерации значение функции стоимости = 0.15262257624385575\n",
      "[[-2.01392821]\n",
      " [-2.13427707]\n",
      " [ 3.12951317]\n",
      " [ 2.83945001]] -1.1968473499129832\n",
      "После 8000 итерации значение функции стоимости = 0.14191204240990557\n",
      "[[-2.11161679]\n",
      " [-2.29357654]\n",
      " [ 3.30715156]\n",
      " [ 3.01578099]] -1.2938987201971492\n",
      "После 9000 итерации значение функции стоимости = 0.1331209645848611\n",
      "[[-2.1966023 ]\n",
      " [-2.44084594]\n",
      " [ 3.46621352]\n",
      " [ 3.1749693 ]] -1.3848973158074593\n",
      "H0:  [[0.59698949 0.94961047 0.94380341 0.04719966 0.15809585 0.16087086\n",
      "  0.60928204 0.94441223 0.11627678 0.01790918 0.50018859 0.05841339\n",
      "  0.71600229 0.84079804 0.97080524 0.1339518  0.63019162 0.64693727\n",
      "  0.04697145 0.03008882 0.98702709 0.6157922  0.10359927 0.05102805\n",
      "  0.90552261 0.01554725 0.31056398 0.89410244 0.18020703 0.99052244]]\n",
      "0.9\n",
      "0    17\n",
      "1    13\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = MyLogisticRegression()\n",
    "model.fit_gradient(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(accuracy_score(Y_pred, Y_test))\n",
    "print(Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc18870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После 0 итерации значение функции стоимости = 1.6401231062112547\n",
      "Weight:  [[ 1.08086355 -1.55820933  0.32943241 -0.30644855]]\n",
      "Bias:  -0.03162277435123163\n",
      "[[0.95639932 0.9750491  0.99615344 0.96953761 0.98490165 0.96558377\n",
      "  0.98436552 0.97114882 0.93520581 0.9459701  0.96044796 0.93340292\n",
      "  0.93726412 0.98400392 0.9853992  0.97123094 0.99141158 0.98141311\n",
      "  0.98779726 0.97518718 0.97992646 0.9757425  0.97169868 0.94121122\n",
      "  0.98549841 0.98648363 0.96188058 0.96642357 0.98211877 0.98682668\n",
      "  0.94225619 0.96546394 0.98434066 0.95110451 0.96923695 0.98984487\n",
      "  0.95577487 0.99843171 0.98774575 0.98164336 0.99384637 0.97018585\n",
      "  0.99470996 0.93327194 0.95367405 0.99582314 0.99180879 0.96798343\n",
      "  0.97783601 0.98252856 0.97769376 0.98850401 0.99124107 0.96646043\n",
      "  0.96915187 0.99240005 0.97336859 0.98662192 0.97974504 0.97973945\n",
      "  0.98452631 0.98009743 0.96009753 0.98611224 0.94867902 0.97436467\n",
      "  0.95988256 0.93965006 0.97466358 0.97466358]]\n",
      "H0:  [[0.57736557 0.98710888 0.99558893 0.0023155  0.02774802 0.03656719\n",
      "  0.71602324 0.99685463 0.01623351 0.0025103  0.67895562 0.00599188\n",
      "  0.8720932  0.89713815 0.99859317 0.06907904 0.74893141 0.67418858\n",
      "  0.00836788 0.00183333 0.99970852 0.60764302 0.05117177 0.00590986\n",
      "  0.97495867 0.00180161 0.07571174 0.98495919 0.02855065 0.99985143]]\n",
      "    Result\n",
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "5      0.0\n",
      "6      1.0\n",
      "7      1.0\n",
      "8      0.0\n",
      "9      0.0\n",
      "10     1.0\n",
      "11     0.0\n",
      "12     1.0\n",
      "13     1.0\n",
      "14     1.0\n",
      "15     0.0\n",
      "16     1.0\n",
      "17     1.0\n",
      "18     0.0\n",
      "19     0.0\n",
      "20     1.0\n",
      "21     1.0\n",
      "22     0.0\n",
      "23     0.0\n",
      "24     1.0\n",
      "25     0.0\n",
      "26     0.0\n",
      "27     1.0\n",
      "28     0.0\n",
      "29     1.0\n",
      "---------\n",
      "133    1\n",
      "103    1\n",
      "120    1\n",
      "95     0\n",
      "94     0\n",
      "89     0\n",
      "72     0\n",
      "130    1\n",
      "60     0\n",
      "50     0\n",
      "68     0\n",
      "80     0\n",
      "123    1\n",
      "83     0\n",
      "140    1\n",
      "54     0\n",
      "126    1\n",
      "127    1\n",
      "62     0\n",
      "81     0\n",
      "105    1\n",
      "138    1\n",
      "76     0\n",
      "92     0\n",
      "119    1\n",
      "65     0\n",
      "90     0\n",
      "146    1\n",
      "59     0\n",
      "122    1\n",
      "Name: target, dtype: int64\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "model2 = MyLogisticRegression()\n",
    "model2.fit_rmsprop(X_train, Y_train)\n",
    "Y_pred = model2.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(\"---------\")\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c53ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1000) (1, 1000)\n",
      "H0:  [[0.52315703 0.44678912 0.47993558 0.45513833 0.50163476 0.48427842\n",
      "  0.46893673 0.43969387 0.50829021 0.50232544 0.50360407 0.52403908\n",
      "  0.50303783 0.46967056 0.51073196 0.46498517 0.49135884 0.47194119\n",
      "  0.53340667 0.48959854 0.46153946 0.47700751 0.45347268 0.5294167\n",
      "  0.50486673 0.49014116 0.48631251 0.44895577 0.52013029 0.44912237\n",
      "  0.50606198 0.48956249 0.49762078 0.44660809 0.49981698 0.52089109\n",
      "  0.50597762 0.4711949  0.48128517 0.46867232 0.50471649 0.52712184\n",
      "  0.42135277 0.51922675 0.50093134 0.4507493  0.44307202 0.52815018\n",
      "  0.53089734 0.47744649 0.48301544 0.45248158 0.50127065 0.474955\n",
      "  0.48474491 0.46774936 0.51211023 0.46968645 0.4577545  0.52892194\n",
      "  0.5041143  0.46218609 0.48623821 0.54659143 0.46210093 0.52849775\n",
      "  0.47141433 0.50818216 0.50851918 0.52570497 0.5177481  0.48527631\n",
      "  0.47616012 0.49710558 0.43419248 0.47453991 0.53208174 0.55384854\n",
      "  0.47517555 0.49994051 0.51710252 0.52828976 0.46848683 0.52753827\n",
      "  0.46597147 0.47284714 0.50200475 0.48177583 0.48149236 0.52072764\n",
      "  0.48725669 0.46488994 0.52283365 0.4883167  0.51644059 0.45072476\n",
      "  0.48924587 0.43444558 0.53273354 0.45946209 0.49024358 0.49991431\n",
      "  0.50630653 0.493852   0.48281253 0.50006719 0.48619772 0.49488595\n",
      "  0.51518913 0.48622767 0.51277301 0.50003442 0.48560859 0.49761468\n",
      "  0.49998931 0.52820283 0.44359278 0.50062303 0.49514726 0.5055175\n",
      "  0.50948824 0.47802481 0.46865599 0.49605635 0.46534186 0.54073751\n",
      "  0.48871389 0.47726989 0.50460402 0.50213052 0.45958298 0.48930992\n",
      "  0.48167189 0.50524055 0.46004665 0.46352331 0.46019232 0.50842855\n",
      "  0.47306772 0.52861515 0.45516844 0.47128552 0.50717091 0.47198296\n",
      "  0.48659414 0.47051328 0.48360234 0.49599716 0.46243617 0.47534776\n",
      "  0.51639497 0.47153878 0.45069355 0.48673341 0.49113562 0.51341646\n",
      "  0.49538419 0.52219487 0.47220913 0.4718819  0.45846646 0.51219312\n",
      "  0.49079569 0.480095   0.48569506 0.5120903  0.49667382 0.45501854\n",
      "  0.49936235 0.45212705 0.51040533 0.47894266 0.50193826 0.4992187\n",
      "  0.50497492 0.48304237 0.49325731 0.51148236 0.47185631 0.4563261\n",
      "  0.45410038 0.47334504 0.49585206 0.54079014 0.46002658 0.48857535\n",
      "  0.48461951 0.45723187 0.48156682 0.46058407 0.49140758 0.47014677\n",
      "  0.53291606 0.51051027 0.47268835 0.4406473  0.48503063 0.481454\n",
      "  0.45630286 0.47301706 0.50150546 0.47160486 0.47188237 0.51400843\n",
      "  0.51343062 0.49751313 0.47259694 0.48654919 0.44584205 0.47423134\n",
      "  0.45332343 0.48366212 0.446458   0.48384607 0.49278542 0.46979356\n",
      "  0.44881505 0.4981873  0.49423928 0.4717654  0.47835234 0.4644546\n",
      "  0.48679971 0.48575399 0.49948998 0.50264804 0.47934209 0.45971572\n",
      "  0.49030837 0.50557986 0.54128392 0.47858473 0.52085522 0.46104217\n",
      "  0.49595494 0.4958761  0.50796756 0.47984001 0.46860122 0.49113218\n",
      "  0.4999648  0.46623466 0.48812436 0.50622654 0.48968694 0.52201235\n",
      "  0.50428629 0.47419651 0.5030121  0.46732872 0.4745209  0.4418581\n",
      "  0.52989152 0.46258416 0.49903119 0.45796228 0.46903354 0.45112514\n",
      "  0.44830555 0.51830652 0.52023754 0.433899   0.45429201 0.4921301\n",
      "  0.52623855 0.47720786 0.47740834 0.49844971 0.47614584 0.49250616\n",
      "  0.48695044 0.48053505 0.4738461  0.52046232 0.54575928 0.49877682\n",
      "  0.48329913 0.52084192 0.49930813 0.44940631 0.46417491 0.47967563\n",
      "  0.4781126  0.47661301 0.46171973 0.48228814 0.4807572  0.50211718\n",
      "  0.49244827 0.50260968 0.51951939 0.5098737  0.52598725 0.47222353\n",
      "  0.4720716  0.44318898 0.52230786 0.47103639 0.49384968 0.48256963\n",
      "  0.46176848 0.4609437  0.47993158 0.5372634  0.48922092 0.47063996\n",
      "  0.48405664 0.49916293 0.49809478 0.49399995 0.48160814 0.53193727\n",
      "  0.46797811 0.49261093 0.49757292 0.5124586  0.45340321 0.53658624\n",
      "  0.50315027 0.48286319 0.53171318 0.48891581 0.4930643  0.48799932\n",
      "  0.49854482 0.49831069 0.51523311 0.48023819 0.5161666  0.49459189\n",
      "  0.47723097 0.50658407 0.46483902 0.48416754 0.51493161 0.43072168\n",
      "  0.50853216 0.49552447 0.5099219  0.52188878 0.46169374 0.46622317\n",
      "  0.53913506 0.49455557 0.45942564 0.46980413 0.47747189 0.51884084\n",
      "  0.5015553  0.44045394 0.53336641 0.5000161  0.49970605 0.48952597\n",
      "  0.4643521  0.50740102 0.44863939 0.54837021 0.49846422 0.50178982\n",
      "  0.48212719 0.51373909 0.52067026 0.50228485 0.47209344 0.49598211\n",
      "  0.50172083 0.51355118 0.47972059 0.50794351 0.46860375 0.47429323\n",
      "  0.49863026 0.48646059 0.44883713 0.53793952 0.46709106 0.50652297\n",
      "  0.4448327  0.49471396 0.52628724 0.50521854 0.49496687 0.46776775\n",
      "  0.50003306 0.52360358 0.50710121 0.50570752 0.43260578 0.46945239\n",
      "  0.45506217 0.51267436 0.49656372 0.47793559 0.47972748 0.4796363\n",
      "  0.50880675 0.49769329 0.46377044 0.4874567  0.46401145 0.52036356\n",
      "  0.4683636  0.53573707 0.46336357 0.50640858 0.46538511 0.47796403\n",
      "  0.48561074 0.46635963 0.52019691 0.45069893 0.45926642 0.48854019\n",
      "  0.49447862 0.49064276 0.50660419 0.51742075 0.51264465 0.51631787\n",
      "  0.47840415 0.49059647 0.48965856 0.45297334 0.46440967 0.51376506\n",
      "  0.4638663  0.4733842  0.47010693 0.53726774 0.50295328 0.4865385\n",
      "  0.49949073 0.50951397 0.45495087 0.47301945 0.53256463 0.4574888\n",
      "  0.46216189 0.44818884 0.46948876 0.48810606 0.46165783 0.46364434\n",
      "  0.50834091 0.48920563 0.50934776 0.49876668 0.53199136 0.53691734\n",
      "  0.50904663 0.50310178 0.53212799 0.47237753 0.45097817 0.5079713\n",
      "  0.48149021 0.46815781 0.45226961 0.47590724 0.50066989 0.47311652\n",
      "  0.5057411  0.53616672 0.45047118 0.47977747 0.4772676  0.49238116\n",
      "  0.46367254 0.50485825 0.5104224  0.49941986 0.45330272 0.43965727\n",
      "  0.48952993 0.47201849 0.43872684 0.48579389 0.47134339 0.50531989\n",
      "  0.5208701  0.50822929 0.45582997 0.49730511 0.48896671 0.5240582\n",
      "  0.49675851 0.451994   0.51422535 0.4712435  0.5255262  0.45512004\n",
      "  0.48950726 0.48919961 0.52242751 0.55446263 0.52271573 0.456399\n",
      "  0.51360815 0.53621193 0.51181958 0.49997189 0.43948409 0.49701213\n",
      "  0.48740012 0.52982255 0.51536954 0.50006459 0.51240799 0.5422811\n",
      "  0.50190663 0.52234344 0.50055863 0.49072504 0.54680907 0.51797324\n",
      "  0.4592452  0.43894003 0.50926551 0.46492294 0.50311301 0.52838202\n",
      "  0.47918415 0.4752669  0.50962095 0.4798251  0.46340182 0.49638271\n",
      "  0.45524002 0.47232297 0.4673735  0.48190063 0.48501228 0.47000182\n",
      "  0.49949692 0.50782179 0.46862978 0.44637077 0.50295223 0.45478629\n",
      "  0.47508675 0.46231381 0.50701667 0.53904049 0.48210321 0.51607801\n",
      "  0.49384136 0.47366042 0.52403305 0.49782808 0.47752813 0.49553279\n",
      "  0.55271504 0.49563301 0.49600014 0.48297964 0.53028402 0.48671399\n",
      "  0.46680688 0.48176761 0.47327163 0.43428164 0.50572174 0.47356368\n",
      "  0.47838483 0.46208924 0.48753624 0.46651771 0.53499222 0.50167939\n",
      "  0.46047216 0.51155238 0.4483394  0.5099596  0.50637918 0.49223742\n",
      "  0.50642987 0.49793134 0.49120623 0.43510036 0.4988931  0.43471116\n",
      "  0.5199007  0.48161031 0.4738358  0.49672298 0.47580771 0.51617743\n",
      "  0.47997696 0.47643514 0.52075827 0.4562958  0.46987633 0.47195521\n",
      "  0.44788608 0.44672236 0.49107645 0.47898907 0.43907475 0.50196598\n",
      "  0.49616984 0.4898562  0.49280478 0.50550068 0.44878259 0.45759568\n",
      "  0.50057578 0.47684093 0.46006629 0.46126198 0.47744701 0.53174415\n",
      "  0.51500322 0.48511537 0.51015879 0.5102974  0.51804421 0.46132864\n",
      "  0.52187757 0.51532291 0.49455578 0.49477271 0.48934797 0.46914017\n",
      "  0.46591486 0.53190597 0.47317696 0.45257504 0.46586846 0.43302334\n",
      "  0.51558579 0.45402662 0.46726831 0.50484139 0.53024773 0.49299657\n",
      "  0.52406779 0.50818368 0.51577375 0.46896548 0.51684543 0.47678844\n",
      "  0.4303129  0.47747524 0.47556565 0.49334947 0.50350771 0.52041349\n",
      "  0.5212153  0.4743234  0.47895328 0.5048592  0.47446653 0.49657157\n",
      "  0.52566716 0.4347616  0.49900727 0.51493979 0.50953843 0.45240238\n",
      "  0.49962474 0.47690272 0.4574048  0.46885205 0.49790866 0.49353721\n",
      "  0.4901846  0.49334253 0.53425915 0.51624028 0.48945375 0.47742214\n",
      "  0.4973425  0.52138438 0.48391836 0.49279817 0.4465982  0.4347706\n",
      "  0.51317085 0.50240795 0.47954814 0.46918454 0.52379083 0.52053689\n",
      "  0.4924835  0.50270403 0.4735388  0.45972484 0.49979303 0.44440138\n",
      "  0.49290446 0.51210219 0.43014107 0.52688022 0.52483937 0.46676841\n",
      "  0.43809624 0.4648947  0.46689249 0.50805251 0.48407262 0.52524211\n",
      "  0.50442621 0.51336292 0.48717519 0.45574175 0.47290378 0.51202508\n",
      "  0.49337604 0.48688477 0.47071286 0.43862916 0.47108299 0.47846309\n",
      "  0.43253718 0.51892111 0.48330363 0.48368697 0.48837379 0.46252224\n",
      "  0.4460819  0.53882766 0.50396997 0.47123162 0.49080541 0.51124229\n",
      "  0.49539072 0.50118295 0.4766191  0.49416301 0.46651178 0.51303596\n",
      "  0.50193925 0.48160866 0.44139303 0.50822554 0.47211482 0.49894586\n",
      "  0.47241248 0.50921504 0.47951093 0.45880099 0.48514602 0.48626891\n",
      "  0.51304204 0.43948422 0.42948542 0.48437824 0.4443253  0.49327499\n",
      "  0.46819286 0.52517176 0.48012247 0.45734152 0.51439813 0.50112766\n",
      "  0.47012398 0.46294003 0.51750131 0.48704837 0.47695716 0.4742448\n",
      "  0.45660109 0.49897654 0.46848495 0.50720325 0.51655744 0.48910158\n",
      "  0.47041023 0.48062833 0.47262448 0.51163133 0.5332695  0.47130366\n",
      "  0.47273776 0.4631691  0.52403832 0.49556149 0.46784228 0.51893006\n",
      "  0.51910282 0.47918742 0.47216142 0.53733887 0.49579257 0.48356376\n",
      "  0.46194184 0.51866151 0.47539347 0.4917257  0.5181792  0.48609308\n",
      "  0.45549687 0.48912112 0.4558489  0.51386234 0.47342123 0.47329457\n",
      "  0.50307699 0.50052235 0.53695219 0.48824164 0.49173661 0.47876844\n",
      "  0.52558958 0.51381221 0.51967737 0.49788176 0.50386528 0.4937834\n",
      "  0.4780062  0.51220778 0.45003132 0.4944368  0.46370128 0.45530676\n",
      "  0.51041692 0.45719842 0.47228198 0.50775778 0.47840973 0.49397248\n",
      "  0.44803056 0.45919798 0.51315605 0.49824653 0.52014027 0.4817344\n",
      "  0.47353682 0.46466599 0.50616693 0.45565089 0.49237957 0.46968316\n",
      "  0.45831567 0.49114741 0.48490569 0.53016416 0.51173228 0.52365615\n",
      "  0.46500111 0.4587331  0.45706673 0.52049846 0.49287484 0.45198293\n",
      "  0.52072489 0.43458478 0.46163372 0.50071297 0.49967337 0.47374677\n",
      "  0.44874399 0.51812265 0.51242993 0.52085281 0.5080727  0.53060501\n",
      "  0.52915066 0.48159085 0.5001108  0.48888791 0.55437583 0.46454483\n",
      "  0.49047293 0.48279552 0.49284421 0.51404681 0.45187878 0.45149588\n",
      "  0.47916222 0.46547701 0.49945334 0.50366649 0.48536049 0.50320871\n",
      "  0.49182781 0.50566961 0.52540137 0.46831205 0.48309334 0.52318167\n",
      "  0.47454316 0.48541613 0.49301729 0.46235178 0.50151049 0.45198039\n",
      "  0.49500763 0.49749804 0.49022758 0.47114373 0.45550922 0.49386931\n",
      "  0.49684033 0.4928712  0.51456117 0.45667873 0.52862728 0.45529965\n",
      "  0.45916048 0.49617871 0.52137517 0.47128322 0.49158153 0.52995158\n",
      "  0.4744462  0.44833094 0.48009937 0.44109826 0.5319709  0.48945257\n",
      "  0.53921449 0.51297477 0.51451999 0.49564309 0.47271398 0.51023605\n",
      "  0.47121427 0.47692938 0.45030845 0.50332218 0.43517277 0.44089505\n",
      "  0.46991104 0.49282729 0.52106627 0.49801459 0.50801504 0.45824556\n",
      "  0.50700966 0.51810862 0.45645685 0.47043934 0.50273648 0.4483636\n",
      "  0.48294566 0.44984603 0.50223776 0.48022906 0.45651724 0.49031615\n",
      "  0.44473779 0.46831466 0.49849387 0.52945364 0.49255109 0.43592834\n",
      "  0.45746937 0.4883281  0.46918792 0.49719252 0.52020014 0.48169186\n",
      "  0.52802081 0.52980246 0.4867829  0.48414192 0.49828023 0.49207261\n",
      "  0.51524416 0.51675998 0.49849479 0.51070468 0.48240807 0.49886318\n",
      "  0.43601194 0.45219212 0.485683   0.47354442 0.50605611 0.48956649\n",
      "  0.51530504 0.49969632 0.47049092 0.52004649 0.47624302 0.45379872\n",
      "  0.47144001 0.47764497 0.4575002  0.46844284 0.47249689 0.47775341\n",
      "  0.50671174 0.49288318 0.49898712 0.52642828 0.49612948 0.51203244\n",
      "  0.50709054 0.4798108  0.50135903 0.51194854 0.51151832 0.46104364\n",
      "  0.51115569 0.47228659 0.47243838 0.45100273]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     57\u001b[0m Y_predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 1]"
     ]
    }
   ],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.v_dw = None\n",
    "        self.v_db = None\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X_train, Y_train, num_iterations):\n",
    "        m = X_train.shape[1]\n",
    "        n = X_train.shape[0]\n",
    "        \n",
    "        # Инициализация параметров и скоростей\n",
    "        self.weight = np.random.randn(1, n)\n",
    "        self.bias = 0\n",
    "        self.v_dw = np.zeros_like(self.weight)\n",
    "        self.v_db = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            # Выполнение шагов обновления весов и смещения с использованием RMSprop\n",
    "            z = np.dot(self.weight, X_train) + self.bias\n",
    "            h0 = self.sigmoid(z)\n",
    "            dw = np.dot(h0 - Y_train, X_train.T) / m\n",
    "            db = np.sum(h0 - Y_train) / m\n",
    "            self.v_dw = self.beta * self.v_dw + (1 - self.beta) * dw**2\n",
    "            self.v_db = self.beta * self.v_db + (1 - self.beta) * db**2\n",
    "            self.weight -= self.learning_rate * dw / (np.sqrt(self.v_dw) + self.eps)\n",
    "            self.bias -= self.learning_rate * db / (np.sqrt(self.v_db) + self.eps)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X_pred):\n",
    "        z = np.dot(self.weight, X_pred) + self.bias\n",
    "        h0 = self.sigmoid(z)\n",
    "        print(\"H0: \", h0)\n",
    "        Y_pred = np.zeros((1, X_pred.shape[1]))\n",
    "        for i in range(h0.shape[1]):\n",
    "            if h0[0, i] <= 0.5:\n",
    "                Y_pred[0, i] = 0\n",
    "            else:\n",
    "                Y_pred[0, i] = 1\n",
    "        Y_pred = Y_pred.flatten()\n",
    "        Y_pred = pd.DataFrame({\"Result\": Y_pred})\n",
    "        return Y_pred\n",
    "\n",
    "# Генерация синтетических данных для демонстрации работы класса\n",
    "X_train = np.random.rand(4, 1000)\n",
    "Y_train = np.random.randint(2, size=(1, 1000))\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# Создание экземпляра класса RMSprop и обучение модели\n",
    "model = RMSprop()\n",
    "model.fit(X_train, Y_train, num_iterations=1000)\n",
    "Y_predict = model.predict(X_train)\n",
    "print(accuracy_score(Y_predict, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccda36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
